input {
	jdbc {
		#数据库连接
        jdbc_connection_string => "jdbc:mysql://192.168.255.10:3306/test?useSSL=false"
		#数据库用户名
        jdbc_user => "root"
		#数据库密码
        jdbc_password => "root"
		#java连接器的jar包位置，应该放在lib\jars\下，否则容易引起未知的错误
        jdbc_driver_library => "/usr/share/logstash/logstash-core/lib/jars/mysql-connector-java-5.1.49.jar"
		#java驱动类名
        jdbc_driver_class => "com.mysql.jdbc.Driver"
		#分页功能
        jdbc_paging_enabled => "true"
		#每次导入数据的页数
        jdbc_page_size => "5000"
		#默认时间区域
        jdbc_default_timezone => "Asia/Shanghai"
        statement => "select * from number"
		#设置监听间隔  各字段含义（由左至右）分、时、天、月、年，全部为*默认含义为每分钟都更新
        schedule => "* * * * *"
		#类型，用于导出时做区分
		#是否把字段名改小写
        lowercase_column_names => false
		#是否记录上次执行结果，true表示会将上次执行结果的tracking_column字段的值保存到last_run_metadata_path指定的文件中
        record_last_run => true
		#使用其他字段比较，而非时间，如果设置true 每次运行的时间 sql_last_value 值不会变化
        use_column_value => false
		#增量时用于判断是否导入的字段，column 必须是递增的
        tracking_column => "updated_at"
		#增量时用于判断是否导入的字段的类型
        tracking_column_type => "timestamp"
		# 是否清除last_run_metadata_path的记录，需要增量同步时此字段必须为false；
        clean_run => false
        # last_run_metadata_path=> "/usr/share/logstash/multi-pipeline/last_run_metadata_path/articles.txt"
		#处理中文乱码问题,可能需要统一编码
        #codec => plain { charset => "GBK"}
    }
}

filter {

}

output {
	# 标准输出，方便debug
	stdout {
		# codec => json_lines
	}
	elasticsearch {
		hosts => ["http://192.168.255.10:9200"]
		# 索引名字，必须小写
		index => "test_number"
		# 如果设置了用户密码，需要更改
		#user => "elastic"
		#password => "changeme"

		# 设置数据表的id为es的主键
		document_id => "%{id}"
		# 使用模板，注意默认为true,一定不能设置为false
		manage_template => true
		# 如果设置为true，模板名字一样的时候，新的模板会覆盖旧的模板
		template_overwrite => true
		# 模板名字，默认logstash
		template_name => "test_number_template"
		template => "/usr/share/logstash/template.json"
  }
}

